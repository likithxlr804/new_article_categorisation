{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) read the parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from newspaper import Article\n",
    "\n",
    "\n",
    "rss_url = 'http://rss.cnn.com/rss/cnn_topstories.rss'\n",
    "feed = feedparser.parse(rss_url)\n",
    "\n",
    "\n",
    "for entry in feed.entries:\n",
    "    article_url = entry.link\n",
    "    print(f\"Fetching article: {entry.title}\")\n",
    "\n",
    "    try:\n",
    "        \n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "       \n",
    "        print(article.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse the article: {e}\")\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) creating necessary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1050 (42S01): Table 'articles_preprocessing' already exists\n",
      "Error: 1050 (42S01): Table 'articles' already exists\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "try:\n",
    "    \n",
    "    cnx = mysql.connector.connect(\n",
    "        user='root',\n",
    "        password='#Likithxlr8',\n",
    "        host='localhost',  \n",
    "        database='new_articles',\n",
    "        auth_plugin='mysql_native_password'\n",
    "    )\n",
    "\n",
    "    \n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "        \n",
    "    create_table_query = \"\"\"\n",
    "        CREATE TABLE articles_preprocessing (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    title VARCHAR(255) NOT NULL UNIQUE,\n",
    "    url VARCHAR(255) NOT NULL UNIQUE,\n",
    "    content TEXT,\n",
    "    published_date DATETIME\n",
    "    \n",
    "    );\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "       \n",
    "    cursor.execute(create_table_query)\n",
    "    print(\"Table 'articles_preprocessing' created successfully or already exists.\")\n",
    "\n",
    "        \n",
    "    cursor.close()\n",
    "        \n",
    "\n",
    "except Error as e:\n",
    "    \n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "   \n",
    "    cnx = mysql.connector.connect(\n",
    "        user='root',\n",
    "        password='#Likithxlr8',\n",
    "        host='localhost',  \n",
    "        database='new_articles',\n",
    "        auth_plugin='mysql_native_password'\n",
    "    )\n",
    "\n",
    "    \n",
    "    cursor = cnx.cursor()\n",
    "\n",
    "       \n",
    "    create_table_query = \"\"\"\n",
    "        CREATE TABLE articles (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    title VARCHAR(255) NOT NULL UNIQUE,\n",
    "    url VARCHAR(255) NOT NULL UNIQUE,\n",
    "    content TEXT,\n",
    "    published_date DATETIME,\n",
    "    category VARCHAR(100)\n",
    "    );\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "    cursor.execute(create_table_query)\n",
    "    print(\"Table 'articles' created successfully or already exists.\")\n",
    "\n",
    "       \n",
    "    cursor.close()\n",
    "        \n",
    "\n",
    "except Error as e:\n",
    "    \n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) insertion operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from newspaper import Article\n",
    "import mysql.connector\n",
    "from mysql.connector import Error, IntegrityError\n",
    "from datetime import datetime, timezone\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'new_articles',\n",
    "    'user': 'root',\n",
    "    'password': '#Likithxlr8',\n",
    "    'auth_plugin':'mysql_native_password'\n",
    "}\n",
    "\n",
    "def insert_article_preprocessing(title, content, url, published_date):\n",
    "    connection = None  \n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        if connection.is_connected():\n",
    "            cursor = connection.cursor()\n",
    "            insert_query = \"\"\"\n",
    "            INSERT INTO articles_preprocessing (title, content, url, published_date)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            data_tuple = (title, content, url, published_date)\n",
    "            cursor.execute(insert_query, data_tuple)\n",
    "            connection.commit()\n",
    "            print(\"Article inserted successfully.\")\n",
    "    except IntegrityError as ie:\n",
    "        print(f\"Error: Duplicate entry for title '{title}'. {ie}\")\n",
    "    except Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        if connection and connection.is_connected():  \n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "def insert_article(title, content, url, published_date,category):\n",
    "    connection = None  \n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        if connection.is_connected():\n",
    "            cursor = connection.cursor()\n",
    "            insert_query = \"\"\"\n",
    "            INSERT INTO articles (title, content, url, published_date,category)\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            data_tuple = (title, content, url, published_date,category)\n",
    "            cursor.execute(insert_query, data_tuple)\n",
    "            connection.commit()\n",
    "            print(\"Article inserted successfully.\")\n",
    "    except IntegrityError as ie:\n",
    "        print(f\"Error: Duplicate entry for title '{title}'. {ie}\")\n",
    "    except Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        if connection and connection.is_connected():  \n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acquiring content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links_url = [\"http://rss.cnn.com/rss/cnn_topstories.rss\", \"http://qz.com/feed\", \"http://feeds.foxnews.com/foxnews/politics\", \"http://feeds.reuters.com/reuters/businessNews\", \"http://feeds.feedburner.com/NewshourWorld\", \"https://feeds.bbci.co.uk/news/world/asia/india/rss.xml\"]\n",
    "for links in links_url:\n",
    "    feed = feedparser.parse(links)\n",
    "    for entry in feed.entries:\n",
    "        title = entry.title\n",
    "        url = entry.link\n",
    "\n",
    "        \n",
    "        if hasattr(entry, 'published'):\n",
    "            published_date = entry.published\n",
    "        else:\n",
    "            \n",
    "            published_date = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        \n",
    "        try:\n",
    "            published_date_obj = datetime.strptime(published_date[:-4], '%a, %d %b %Y %H:%M:%S')\n",
    "            published_date_obj = published_date_obj.replace(tzinfo=timezone.utc)  # Set timezone to UTC\n",
    "            published_date_str = published_date_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing date: {e}\")\n",
    "            continue  \n",
    "\n",
    "        \n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            \n",
    "            content = article.text\n",
    "            \n",
    "\n",
    "            \n",
    "            if content:\n",
    "                insert_article_preprocessing(title, content, url, published_date_str)\n",
    "            else:\n",
    "                print(f\"No content found for article: {title}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article '{title}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read from data base (mysql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "def read_data_from_table():\n",
    "    try:\n",
    "        \n",
    "        connection = mysql.connector.connect(\n",
    "            host=\"localhost\",      \n",
    "            user=\"root\",\n",
    "            password=\"#Likithxlr8\",\n",
    "            database=\"new_articles\",\n",
    "            auth_plugin='mysql_native_password'\n",
    "        )\n",
    "\n",
    "       \n",
    "        cursor = connection.cursor()\n",
    "\n",
    "       \n",
    "        query = \"SELECT * FROM articles_preprocessing\"\n",
    "\n",
    "        \n",
    "        cursor.execute(query)\n",
    "\n",
    "        \n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        \n",
    "        nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "        \n",
    "        categories = {\n",
    "                \"Terrorism/Protest/Unrest\": [\"terrorism\", \"protest\", \"riot\", \"demonstration\", \"violence\", \"unrest\", \"extremism\", \"radicalization\", \"revolution\", \"insurgency\"],\n",
    "                \"Politics\": [\"politics\", \"government\", \"election\", \"democracy\", \"parliament\", \"politician\", \"policy\", \"legislation\", \"campaign\", \"diplomacy\"],\n",
    "                \"Natural Disasters\": [\"earthquake\", \"flood\", \"hurricane\", \"tornado\", \"tsunami\", \"wildfire\", \"landslide\", \"volcano\", \"cyclone\", \"drought\"],\n",
    "                \"Technology\": [\"innovation\", \"artificial intelligence\", \"software\", \"hardware\", \"startup\", \"cloud computing\", \"cybersecurity\", \"gadgets\", \"data\", \"machine learning\"],\n",
    "                \"Business\": [\"business\", \"commerce\", \"enterprise\", \"startup\", \"economy\", \"finance\", \"investment\", \"market\", \"trade\", \"corporation\"],\n",
    "                \"Entertainment\": [\"celebrity\", \"movie\", \"film\", \"music\", \"television\", \"concert\", \"award\", \"gossip\", \"theater\", \"festival\"],\n",
    "                \"Sports\":[\"Soccer (Football)\", \"Cricket\", \"Basketball\", \"Field Hockey\", \"Tennis\", \"Volleyball\", \"Table Tennis\", \"Baseball\", \"American Football\", \"Golf\"]\n",
    "\n",
    "            }\n",
    "\n",
    "        \n",
    "        for row in results:\n",
    "            title = row[1]           \n",
    "            url = row[2]              \n",
    "            content = row[3]          \n",
    "            published_date = row[4]   \n",
    "            \n",
    "            \n",
    "            text = content\n",
    "            doc = nlp(text)\n",
    "            doc_vector = doc.vector\n",
    "            \n",
    "\n",
    "            def compute_similarity(text_vector, category_keywords, nlp):\n",
    "                keyword_vectors = [nlp(keyword).vector for keyword in category_keywords]\n",
    "                mean_keyword_vector = np.mean(keyword_vectors, axis=0)  \n",
    "                similarity_score = cosine_similarity([text_vector], [mean_keyword_vector])\n",
    "                return similarity_score[0][0]\n",
    "\n",
    "           \n",
    "            category_scores = {}\n",
    "            for category, keywords in categories.items():\n",
    "                category_scores[category] = compute_similarity(doc_vector, keywords, nlp)\n",
    "\n",
    "            max_key = max(category_scores, key=category_scores.get)\n",
    "            print(\"Category Scores:\", category_scores)\n",
    "            if content:\n",
    "                insert_article(title, content, url, published_date,max_key)\n",
    "            else:\n",
    "                print(f\"No content found for article: {title}\")\n",
    "\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "\n",
    "    finally:\n",
    "       \n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if connection:\n",
    "            connection.close()\n",
    "\n",
    "\n",
    "read_data_from_table()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
